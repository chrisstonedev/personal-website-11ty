---
title: 'Deploy Without Fear Using Automated Tests'
description: A conference talk about the usefulness of different types of automated tests.
layout: reveal.njk
---

<section data-autoslide="15000">
	<h1 class="r-fit-text">Deploy Without Fear<br />Using Automated Tests</h1>
	<div
		style="display: grid; grid-template-columns: 2fr 1fr; align-items: center"
	>
		<div>
			<p>Chris Stone<br />Senior Software Engineer<br />Flashlight Learning</p>
			<p><a href="/">https://chrisstone.dev</a></p>
		</div>
		<a href="/deploy-without-fear"
			><img
				src="/img/deploy-without-fear-2d-code.svg"
				alt="QR code for talk notes and slides"
		/></a>
	</div>
	<aside class="notes">
		Hello everyone! My name is Chris Stone. I am a full stack developer and I
		also write tests. I wanted to talk today about some of the reasons I enjoy
		writing tests and the things I value when I do write tests. There are
		several reasons I enjoy writing tests, but most of all, the biggest reason I
		enjoy
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<ul>
		<li>I get scared üò≠</li>
		<li>I see code I don't understand</li>
		<li>I'm worried the deployment had problems</li>
		<li>I'm sure I'll forget to test something manually</li>
	</ul>
	<img
		src="/img/deploy-without-fear-slides-im-scared.png"
		alt="I don't know what any of this is and I'm scared"
	/>
	<aside class="notes">
		I can get really scared. Anxious, nervous, worried, all of it. Maybe I'm
		asked to maintain code I have never seen before and is really high risk. I
		also have tech trauma. I have had so many times when I merged in code that I
		was so sure was completely fine, only to accidentally cause a major issue,
		and now, I look at that "squash and merge" button and start panicking. And
		maybe there are a set of steps that I am advised to always test when a new
		release is made, but I can be forgetful. Maybe I take on a code review, and
		I say, "looks good to me!" and it causes a problem I should have caught
		because I only tested 19 out of 20 possible scenarios.
	</aside>
</section>
<section data-auto-animate data-auto-animate-restart data-autoslide="15000">
	<h2>Dealing with fear</h2>
	<blockquote style="width: 100%; font-size: 28pt">
		I must not fear.
		<strong
			>Fear is the mind-killer. Fear is the little-death that brings total
			obliteration.</strong
		>
		I will face my fear. I will permit it to pass over me and through me. And
		when it has gone past I will turn the inner eye to see its path. Where the
		fear has gone there will be nothing. Only I will remain.
	</blockquote>
	<figure
		style="
			display: grid;
			grid-template-columns: 1fr 1fr;
			gap: 2em;
			justify-items: center;
			align-items: center;
		"
	>
		<img
			src="/img/deploy-without-fear-slides-dune.jpg"
			alt="Dune by Frank Herbert"
			style="height: 250px"
		/>
		<figcaption><em>Dune</em> by Frank Herbert</figcaption>
	</figure>
	<aside class="notes">
		We have to do something about that! Fear is the mind-killer after all.
		Surely there is something we can do to save ourselves from these little
		deaths.
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Dealing with fear</h2>
	<div
		style="display: grid; grid-template-columns: 1fr 1fr; align-items: center"
	>
		<blockquote style="background: aliceblue">
			Write tests until fear is transformed into boredom.
		</blockquote>
		<figure
			style="
				display: grid;
				grid-template-columns: 1fr 1fr;
				justify-items: center;
				align-items: center;
			"
		>
			<img
				src="/img/deploy-without-fear-slides-tdd-by-example.jpg"
				alt="Test-Driven Development by Example by Kent Beck"
				style="height: 200px"
			/>
			<figcaption style="font-size: x-large">
				<em>Test-Driven Development<br />By Example</em><br />by Kent Beck
			</figcaption>
		</figure>
	</div>
	<p>
		If something in our application scares me,<br />that's where I write a test.
	</p>
	<p>
		If I don't worry about something not working,<br />then I don't worry about
		testing any further.
	</p>
	<aside class="notes">
		The book Test-Driven Development by Kent Beck contains a well-known quote:
		Write tests until fear is transformed into boredom. Whenever people ask me,
		how do you know what makes a good test? Where should you start writing a
		test? Once you start, how do you know when to finish? Well, it's kind of an
		oversimplified answer, but... I keep coming back to this quote. Write tests
		until fear is transformed. In other words... If something in our application
		scares me, that's where I write a test. If I don't worry about something not
		working, then I don't worry about testing any further. Again, this is
		overgeneralizing what is honestly a difficult problem of how and why to
		write tests, but I really like this as a starting point, and I'll show some
		examples of how I use this on my team throughout the talk.
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<h3>Regression detection</h3>
	<ul>
		<li>A good test is possible to fail someday</li>
		<li>Tests should run on every commit in CI/CD</li>
	</ul>
	<aside class="notes">
		Another reason is regression detection. A good test is one that could
		conceivably fail. Imagine writing a test that tests that adding two numbers
		together returns the sum. Addition will always be the same. A good test will
		test something valuable and meaningful, something that would scare us if it
		did go wrong. By putting these tests in our CI/CD pipelines, we get near
		instant feedback if we have a problem so we can know the exact commit in
		which the problem originated.
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<h3>Regression detection</h3>
	<p>Address flaky tests</p>
	<p>Team should have faith in the test suite</p>
	<ul>
		<li>Fix flakiness if you can</li>
		<li>Skip until you get the time, if necessary</li>
	</ul>
	<aside class="notes">
		Also, once you do get tests in a CI/CD pipeline, if tests fail, take those
		failures seriously. If you throw your hands up and say, "it's ok, some of
		these just fail all the time," the team will lose faith in the test suite,
		which will lead to people no longer wanting to spend time maintaining them
		("they always fail, why bother trying to fix them") and also cause alert
		fatigue ("the build always fails, it's not worth checking if a serious
		problem is happening, it's probably just flakiness"). If a test fails for a
		bad reason, try to get the time to fix it to restore intended functionality
		if you can. And worst case scenario, just skip the test until you
		<em>can</em> get that time.
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<h3>Avoid writing superfluous code</h3>
	<ul>
		<li>Test-driven development (TDD)</li>
		<li>
			Red-green-refactor cycle will help drive simple design to avoid accidental
			complexity
			<ul>
				<li>Remember, refactoring preserves behavior!</li>
			</ul>
		</li>
	</ul>
	<aside class="notes">
		<p>
			Another reason I enjoy tests is because I want to accomplish what I need
			while writing the least amount of code as possible. I don't want to be
			clever, I just want to get the job done in a readable and maintainable
			way. Test-driven development can be a tough thing to wrap one's head
			around if they have not used it before, and it's not a subject to which I
			want to devote much time today, but the basics is that you write a test
			first of the behavior that you want to see given a certain action. That
			test will not pass at first; in other words...
		</p>
		<ul>
			<li>
				Red. Then, you write <em>only</em> enough code that makes the test pass,
				which gets you to...
			</li>
			<li>
				Green. That's it. Pencils down. If you made the test pass, you did
				enough. You would then add to that test to make it fail again or maybe
				write a new test that would fail, but not before you consider time to
			</li>
			<li>
				Refactor. As long as it was green before and stays green, you can spend
				this time to simplify the design of the code you have written as long as
				all tests stay green.
			</li>
			<li>
				As a reminder, the definition of refactoring is to improve the design of
				existing code without changing the behavior! If you change how the app
				works while you are refactoring, you are no longer refactoring, you are
				instead changing how the app works. That's just a quick conceptual
				overview of TDD, but I hope it makes sense how writing tests may
				actually lead to code that does just what is necessary without doing too
				much.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<h3>Tests are the best documentation</h3>
	<div style="display: grid; grid-template-columns: 1fr 1fr">
		<img
			src="/img/confluence.png"
			alt="Confluence screenshot"
			style="
				height: 300px;
				width: 350px;
				object-fit: cover;
				object-position: 0 0;
			"
		/>
		<pre
			style="width: 600px"
		><code data-trim data-noescape data-line-numbers class="language-ts">
it("should only alert when less than average", () => {
	const mockData = [1, 2, 3];
	const input = 2;
	const actual = isLevelInDanger(
		mockData, input);
	assert.deepStrictEqual(actual, false);
});
  	</code></pre>
	</div>
	<aside class="notes">
		Another reason I write test is because I want very good documentation of my
		code. I think it is great to document decisions around our code and help
		others explain why things are the way they are. But I'm going to explain why
		I personally think executable documentation is the best documentation. First
		of all, consider a huge Confluence document. Having documentation that is
		far removed from the code means it is less likely for people to encounter it
		in their standard work. Platforms like this can be very helpful for planning
		meetings and for documenting decisions and moment-in-time reflections. But
		if the purpose is to say, "this is how this part of the system works," I
		guarantee, there will come a day when that Confluence page is no longer
		telling the truth, and someone is going to waste their time trying to
		reconcile a misconception.
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Why do I write tests?</h2>
	<h3>Tests are the best documentation</h3>
	<h4>What does it mean for writing to be effective?</h4>
	<ul>
		<li>Readable</li>
		<li>Relevant</li>
		<li>Applicable</li>
	</ul>
	<aside class="notes">
		<p>
			And just like any document we would write for any purpose, we have several
			considerations we need to make. The act of writing is usually intended for
			some audience to read, whether it is ourselves or others.
		</p>
		<ul>
			<li>
				For one, the things we write should be readable. Just like with any code
				that we write, we optimize for readability. We know that code that just
				works but is difficult for a human to interpret is bad quality and that
				we can do better.
			</li>
			<li>
				The things we write should also be relevant. Going back to the fear
				thought, we should only be testing things that are worth the execution
				time when running and are worth the human time of reading. Imagine if we
				had a bunch of UI tests that do nothing more than mount a component,
				assuming success because nothing was actually asserted. We would be
				better off removing those entirely so that someone trying to find
				meaningful tests finds something meaningful.
			</li>
			<li>
				Our writing should also be applicable. If we have code that is too
				difficult to refactor, but we were able to get a test around it, then
				others who will need to test in this area will find that very useful. On
				the other hand, a test that we write for ourselves that is not likely to
				ever fail and is not likely to instill fear in others should make us
				think twice about keeping it. It is totally acceptable to delete tests
				from our suite that no longer serve us.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-auto-animate-restart data-autoslide="15000">
	<h2 style="font-size: xxx-large">Types of tests</h2>
	<div style="display: grid; grid-template-columns: 1fr 1fr">
		<div>
			<h3>Unit tests</h3>
			<ul>
				<li>Tests a small unit of isolated simple design code</li>
				<li>
					Mocks out any calls to an external dependency, data layer, or API
					interface
				</li>
				<li>Runs in milliseconds</li>
			</ul>
		</div>
		<div>
			<h3>Integration tests</h3>
			<ul>
				<li>
					A relatively small area of code that is allowed to talk to other
					components or a test database interface
				</li>
				<li>Runs up to a few seconds</li>
			</ul>
		</div>
	</div>
	<aside class="notes">
		<p>
			There are dozens of kinds of tests, and many people will give you all
			kinds of definitions. All I can tell you today is that these are the
			definitions I have used. Some may disagree, but this is how I categorize
			most kinds of tests, whether they are back-end, front-end, or even
			infrastructure-focused.
		</p>
		<ul>
			<li>
				A unit test is one that tests a tiny amount of isolated simple design
				code, mocking out any call to an external dependency, data layer, or API
				interface. As a result, they should run very fast, in terms of
				milliseconds.
			</li>
			<li>
				Integration tests will also test a relatively small area of code, but it
				is allowed to talk to other components or a test database interface. As
				a result, it may take up to a few seconds to run.
			</li>
			<li>
				End-to-end tests cover a full user behavior journey using browser
				protocols or emulated devices. These tests will take the longest, taking
				tens of seconds on average to run.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-auto-animate-restart data-autoslide="15000">
	<h2 style="font-size: xxx-large">Types of tests</h2>
	<h3>End-to-end tests</h3>
	<ul>
		<li>
			Tests a full user behavior journey using browser protocols or emulated
			devices
		</li>
		<li>Runs up to tens of seconds</li>
		<li>Takes time to code and and run</li>
		<li>Potentially good start point in a cluttered code base</li>
	</ul>
	<aside class="notes">
		<p>
			There are dozens of kinds of tests, and many people will give you all
			kinds of definitions. All I can tell you today is that these are the
			definitions I have used. Some may disagree, but this is how I categorize
			most kinds of tests, whether they are back-end, front-end, or even
			infrastructure-focused.
		</p>
		<ul>
			<li>
				A unit test is one that tests a tiny amount of isolated simple design
				code, mocking out any call to an external dependency, data layer, or API
				interface. As a result, they should run very fast, in terms of
				milliseconds.
			</li>
			<li>
				Integration tests will also test a relatively small area of code, but it
				is allowed to talk to other components or a test database interface. As
				a result, it may take up to a few seconds to run.
			</li>
			<li>
				End-to-end tests cover a full user behavior journey using browser
				protocols or emulated devices. These tests will take the longest, taking
				tens of seconds on average to run.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Types of tests</h2>
	<h3>Which type to pick</h3>
	<div style="display: grid; grid-template-columns: 1fr 1fr" class="r-stretch">
		<figure>
			<img
				src="/img/deploy-without-fear-slides-test-pyramid.png"
				alt="Test Pyramid"
				style="height: 250px"
			/>
			<figcaption>Test Pyramid</figcaption>
		</figure>
		<figure>
			<img
				src="/img/deploy-without-fear-slides-test-trophy.jpg"
				alt="Test Pyramid"
				style="height: 250px"
			/>
			<figcaption>Test Trophy</figcaption>
		</figure>
	</div>
	<p>
		<strong>Pick the lowest level that gives value.</strong>
	</p>
	<aside class="notes">
		<p>
			That alone sort of simplifies the whole concept, and it's possible to
			consider some gray area middle ground. And you could even technically pick
			any one of these tools and be better off in situations than you'd be
			without any of them. But why you'd go for one over another depends on
			various considerations.
		</p>
		<ul>
			<li>
				Mike Cohn's Testing Pyramid is referenced by many people to illustrate
				how, starting from unit tests and going up to UI or end-to-end tests,
				the execution gets slower as more integration is required. The idea is
				that the level of investment at each level should go down the further
				you go up, focusing on unit tests.
			</li>
			<li>
				Kent C. Dodds' Testing Trophy is another perspective, though. Some
				projects may not have much complicated business logic that is worth
				testing in isolation, but the integration between components could be a
				vital layer. I think both of these approaches give us interesting things
				to consider. Ultimately, what I tell people is to not worry too much
				about whether you're investing too much in one area or not, and instead,
			</li>
			<li>
				confirm that, when you go to write a test, you are picking the lowest
				possible level that gives you value for the behavior that you need to
				test, the behavior that would give you fear until you tested it.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h3>Which type to pick</h3>
	<pre
		style="font-size: x-large"
	><code data-trim data-noescape data-line-numbers class="language-ts">
export function isLevelInDanger(oldScores: number[],
		newScore: number) {
	const sum = oldScores.reduce((a, b) => a + b);
	return newScore <= (sum / oldScores.length);
}
  </code></pre>
	<pre
		style="font-size: x-large"
	><code data-trim data-noescape data-line-numbers data-ln-start-from="12">
if input != nil {
    if input.ResourceType.IsValid() {
       query = query.Where("static_resource_type = ?", input.ResourceType)
    }
    if len(input.Grades) > 0 {
       query = query.Where("grade IN (?)", input.Grades)
    }
    if len(input.SubjectAreas) > 0 {
       query = query.Joins(`INNER JOIN subject_area_resource_mappings ON
  </code></pre>
	<table data-id="test-types-table">
		<thead>
			<tr>
				<th style="text-align: center">Unit</th>
				<th style="text-align: center">Integration</th>
				<th style="text-align: center">End-to-end</th>
			</tr>
		</thead>
		<tbody>
			<tr>
				<td style="text-align: center">‚ùì</td>
				<td style="text-align: center">‚ùì</td>
				<td style="text-align: center">‚ùì</td>
			</tr>
		</tbody>
	</table>
	<aside class="notes">
		<p>
			Let's start with this example again. I know this is overly simplistic, but
			imagine a function kind of like this that we did want to test. Let's say
			we were doing some super complicated mapping and reducing here.
		</p>
		<ul>
			<li>
				It wouldn't take much for us to write a unit test around this. We could
				even very easily have parameterized test cases to test several
				conditions.
			</li>
			<li>
				An integration test wouldn't take too long either, let's say the calling
				function wanted to fetch those scores from the database, so we could
				seed it and then set our expectation based on what we expect it to get.
			</li>
			<li>
				An end-to-end test is possible, but the level of effort required to put
				it together will be a very hard sell, and parameterizing it
				significantly may be a no-go.
			</li>
			<li>
				That said, if we did put it together, and worked hard at it, we could
				prove that it works.
			</li>
			<li>
				But we could do that with unit and integration too. So our choice is
				between a test that will take milliseconds to run in memory or a few
				seconds to stand up a test database, seed it, and then close the
				connection.
			</li>
			<li>In this case, we get the value we really need from the unit test.</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-auto-animate-restart data-autoslide="15000">
	<h2>Considerations in back-end tests</h2>
	<ul>
		<li>
			The more we prepare data in the backend for how the frontend will use it,
			the more we can rely on quick back-end unit tests.
		</li>
		<li>
			Difference between a unit and integration test may just be whether an
			interface is <strong>mocked</strong> or if its
			<strong>implementation</strong> is actually used.
		</li>
	</ul>
	<aside class="notes">
		<p>
			One nice thing about this project is that there were back-end tests, just
			not a lot of them. But at least it gave us a starting point.
		</p>
		<ul>
			<li>
				Something to consider in general with back-end development is that, if
				you know how the front-end will interact with this data, and you can do
				all of your in-memory modifications in the back-end, then you can unit
				test those modifications in what will generally be your fastest tests to
				run of all.
			</li>
			<li>
				There are several libraries out there for all kinds of languages, and
				the tool is not going to be what determines if it is unit or
				integration, it is whether dependencies behind interfaces are mocked or
				if we actually allow the code to hit a test database or a test API.
			</li>
			<li>
				Our back-end is Go, and the project used Testify, which we continue to
				use; it is a great library. On .NET projects, I have used NUnit and
				xUnit. There are many options for your needs.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Adding end-to-end tests</h2>
	<pre><code data-trim data-line-numbers="11-14,20" data-id="endToEndSample" class="language-ts">
test('teacher can create new assignment',
		async ({ page }) => {
	await this.page.goto('/login');
	await this.page
      .locator('.MuiCircularProgress-svg')
      .waitFor({ state: 'hidden' });
	await this.page.getByRole('combobox').fill(districts.newDistrict.districtName);
	await this.page
		.getByRole('option', { name: district, exact: true })
		.click();
	await this.page.getByRole('textbox', { name: 'Username' }).fill(users.teacher.email);
	await this.page.getByRole('textbox', { name: 'Password' }).fill(process.env.USER_PASSWORD!);
	await this.page.getByRole('button', { name: 'Login' }).click();
	await expect(this.page.getByLabel('account of current user')).toContainText(
		`${users.superAdmin.firstName} ${users.superAdmin.lastName}`,
	);
	await this.page.getByRole('button', { name: 'create Assignment' }).click();
	await this.page
		.getByRole('main')
		.getByRole('button', { name: '‚Äã', exact: true })
		.click();
	await this.page.getByText(className).click();
	// ...
});
  	</code></pre>
	<p>We started with Playwright Codegen</p>
	<aside class="notes">
		<p>
			Starting is actually very easy. Once you have set up a Playwright project,
			while there is certainly a learning curve to understand how to best use
			it, it is possible to dive right into creating tests using their test code
			generation tool. It will open up a browser and record all actions you take
			and generate a test for you.
		</p>
		<ul>
			<li>
				As you can see, it even tries to use semantic information from your page
				to determine reliable locators.
			</li>
			<li>
				But you can still get weird results if your app is not accessible. In
				this case, I am trying to click on what I think is a drop-down select
				element, but it is actually constructed of other components, where the
				drop-down arrow is a button that has no text, so we look for a button
				with absolutely no text. Just looking at this should be a red flag
				because, if our test can't target anything more specific about this
				button and its perceived role, then what hope does someone using a
				screen reader have? But this is the best it can do for now, so we make
				note of that for our accessibility audit we aim to have.
			</li>
			<li>
				But at least we have a test now. But it's not very readable, is it? It
				could be much, much worse, but if someone wanted to know, what is the
				purpose of this test, it might take a while to truly understand.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Adding end-to-end tests</h2>
	<pre><code data-trim data-line-numbers data-id="endToEndSample" class="language-ts">
test('teacher can create new assignment',
		async ({ loginPage, header, teacherPage }) => {
	await loginPage.login(
		districts.newDistrict.districtName,
		users.teacher.email,
		process.env.USER_PASSWORD!,
	);
	await expect(header.usernameButton).toContainText(
		`${users.teacher.firstName} ${users.teacher.lastName}`,
	);
	await teacherPage.createNewAssignment();
	await teacherPage.chooseClass(classes.newClass.className);
	// ...
});
  </code></pre>
	<p>We refactored with Page Object Model</p>
	<aside class="notes">
		<p>
			So we then took some time and refactored our test code to using the Page
			Object Model pattern. We are performing the exact same operations here as
			before, but everything related to logging in can be abstracted into a
			function, and the idea of clicking a button called Create to create an
			assignment could benefit from some readability. It's in a better direction
			now. Page Object Model always feels a little bit controversial to me
			though, because if we just abstract every single line, then we are hiding
			the true functionality, and it's easy now to change what these functions
			do without remembering to change their names. But I still ultimately enjoy
			this pattern because it does increase readability, and we can understand
			the intent of a test much easier.
		</p>
		<ul>
			<li>
				One of our next big changes to the app was around the login process. We
				got about a dozen tests up and running, and every single one of them
				needed to go to a brand-new session and login as a user all over again.
				It can be very time intensive, not to mention it looks like suspicious
				behavior.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Adding end-to-end tests</h2>
	<pre><code data-trim data-line-numbers data-id="endToEndSample" class="language-ts">
test.use({ storageState: StorageStates.teacher });
test('teacher can create new assignment',
		async ({ teacherPage }) => {
	await teacherPage.page.goto('/');
	await teacherPage.createNewAssignment();
	await teacherPage.chooseClass(classes.newClass.className);
	// ...
});
  	</code></pre>
	<p>Took advantage of Storage State</p>
	<aside class="notes">
		<p>
			So we took advantage of the storage state feature. We have a few users we
			may need to log in as for various tests, so before any actual tests get
			run, we log in as each of these users and save the state of the browser
			context.
		</p>
		<ul>
			<li>
				Then, all of our tests simply need to call out which state they need,
				and the test starts with them already authenticated. This saves us a lot
				of time when running our tests.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Adding front-end tests</h2>
	<pre><code data-trim data-line-numbers="19-25,27">
const keywords: MockedResponse&lt;GetKeywordsQuery> = {
	request: {
		query: GetKeywordsDocument,
	},
	result: {
		data: {
			keyword: [{ id: '1234', keyword: 'hello' }],
		},
	},
};
it('should add new keyword', async () => {
	const onChange = vi.fn();
	render(
		&lt;MockedProvider mocks={[keywords]}>
			&lt;AddKeywordTags onChange={onChange} />
		&lt;/MockedProvider>
	);

	const combobox = screen.getByRole('combobox');
	await waitFor(() => expect(combobox).not.toBeDisabled(), {
		timeout: 500,
	});
	await userEvent.type(combobox, 'world');
	expect(screen.getByDisplayValue('world')).toBeInTheDocument();
	await userEvent.keyboard('{enter}');
	expect(onChange).not.toHaveBeenCalled();
	expect(screen.getByText('Add a new keyword')).toBeVisible();
});
  </code></pre>
	<aside class="notes">
		<p>
			On my current team, we inherited a project that had all kinds of front-end
			components but not a single front-end test. It was something that always
			bothered us, and it did not make sense for us to go in and start adding
			hundreds of tests when we could be working on feature work. But when we
			started tackling something that behaved a bit strange in the front-end, it
			was our opportunity.
		</p>
		<ul>
			<li>
				Notice that we are trying to keep this behavior-focused. We want to
				think of this less as what data outputs do we want for given data
				inputs, but rather what result should a user see when they interact in
				certain ways. As much as you can help it, you want your tests to focus
				on the behavior, not the implementation details.
			</li>
			<li>
				Sometimes, though, it can be hard to truly get out of implementation
				details. This here is not ideal; we are mocking a function and asking
				about whether it got called. The reason for this is that the way this
				component had been written, this would be incorporated on a form, and it
				would manage all change events and form states. We could do this another
				way if we wanted to test the entire form on which this component is
				used, but that is more than we wanted to test for this story.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-autoslide="15000">
	<h2>Adding front-end tests</h2>
	<pre><code data-trim data-line-numbers="12,15,26">
const keywords: MockedResponse&lt;GetKeywordsQuery> = {
	request: {
		query: GetKeywordsDocument,
	},
	result: {
		data: {
			keyword: [{ id: '1234', keyword: 'hello' }],
		},
	},
};
it('should add new keyword', async () => {
	const onChange = vi.fn();
	render(
		&lt;MockedProvider mocks={[keywords]}>
			&lt;AddKeywordTags onChange={onChange} />
		&lt;/MockedProvider>
	);

	const combobox = screen.getByRole('combobox');
	await waitFor(() => expect(combobox).not.toBeDisabled(), {
		timeout: 500,
	});
	await userEvent.type(combobox, 'world');
	expect(screen.getByDisplayValue('world')).toBeInTheDocument();
	await userEvent.keyboard('{enter}');
	expect(onChange).not.toHaveBeenCalled();
	expect(screen.getByText('Add a new keyword')).toBeVisible();
});
  </code></pre>
	<aside class="notes">
		<p>
			On my current team, we inherited a project that had all kinds of front-end
			components but not a single front-end test. It was something that always
			bothered us, and it did not make sense for us to go in and start adding
			hundreds of tests when we could be working on feature work. But when we
			started tackling something that behaved a bit strange in the front-end, it
			was our opportunity.
		</p>
		<ul>
			<li>
				Notice that we are trying to keep this behavior-focused. We want to
				think of this less as what data outputs do we want for given data
				inputs, but rather what result should a user see when they interact in
				certain ways. As much as you can help it, you want your tests to focus
				on the behavior, not the implementation details.
			</li>
			<li>
				Sometimes, though, it can be hard to truly get out of implementation
				details. This here is not ideal; we are mocking a function and asking
				about whether it got called. The reason for this is that the way this
				component had been written, this would be incorporated on a form, and it
				would manage all change events and form states. We could do this another
				way if we wanted to test the entire form on which this component is
				used, but that is more than we wanted to test for this story.
			</li>
		</ul>
	</aside>
</section>
<section data-auto-animate data-auto-animate-restart data-autoslide="15000">
	<div style="display: grid; grid-template-columns: 1fr 1fr; width: 100%">
		<div style="width: 550px">
			<h2>Adding to CI/CD</h2>
			<pre
				style="font-size: 18px"
			><code data-trim data-line-numbers="7" class="language-yaml">
jobs:
  e2e:
    name: Run the e2e tests
    uses: ./.github/workflows/_shared_run_e2e_tests.yml
    with:
      ENVIRONMENT: prod
    needs: [deploy-api, deploy-web-app]
    secrets:
      E2E_USER_PASSWORD: $&#123;{ secrets.E2E_USER_PASSWORD }}
      SLACK_WEBHOOK_URL: $&#123;{ secrets.SLACK_WEBHOOK_URL }}
	</code></pre>
		</div>
		<div style="width: 375px">
			<pre style="font-size: 18px"><code data-trim class="language-yaml">
jobs:
  build:
    name: Lint and Test Web
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
		</code></pre>
			<pre
				style="font-size: 18px"
			><code data-trim data-line-numbers="1-4,11-14" data-ln-start-from="20" class="language-yaml">
      - name: Lint for web-app
        run: |
          cd packages/apps/web
          yarn lint

      - name: Build for web-app
        run: |
          cd packages/apps/web
          yarn build

      - name: Test for web-app
        run: |
          cd packages/apps/web
          yarn test
		</code></pre>
		</div>
	</div>
	<aside class="notes">
		<p>
			Tests that require manual runs are easy to get ignored and your team can
			miss out on valuable feedback, so once you start getting some tests
			together, even if it's just your very first one, you want to start getting
			this in an automated run.
		</p>
		<ul>
			<li>
				In our case, we actually already had a CI pipeline that made sure that
				every pull request to main went through a build process to verify that
				the app was in a good state.
			</li>
			<li>So with that, we were able to add our testing to this step now.</li>
			<li>
				We also, for the first time, added lint checks. When we first ran a
				linter in our project, we received many errors and it kind of seemed
				overwhelming to start taking action on them, so we downgraded known
				project errors to warnings so that new errors will fail this action
				first before we even get to the build, and if both the lint and build
				succeeds, the test is the last line of defense before we allow a PR to
				be merged.
			</li>
		</ul>

		<p>
			As for end-to-end tests, we have them run on every deployment to every
			environment, including our staging and production environments.
		</p>
		<ul>
			<li>
				We set this job to need the deployment of the main jobs that the app
				requires to function so it does not run any sooner.
			</li>
			<li>
				We use an environment secret for a user password that gets used to
				authenticate in the tests,
			</li>
			<li>
				and we have a repository secret for a Slack webhook that immediately
				posts results, passing or failing.
			</li>
		</ul>
	</aside>
</section>
<section>end</section>
